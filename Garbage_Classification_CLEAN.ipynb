{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c35745c3-8bf2-4ed1-8229-9dec09750204",
   "metadata": {
    "id": "c35745c3-8bf2-4ed1-8229-9dec09750204"
   },
   "source": [
    "![image.png](attachment:224e11e2-3371-414e-b86c-0661cb7c7c93.png)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16342803-fc0b-4de6-83db-7e1d1a8a353f",
   "metadata": {
    "id": "16342803-fc0b-4de6-83db-7e1d1a8a353f"
   },
   "source": [
    "\n",
    "\n",
    "# Garbage Classification with EfficientNetV2B2\n",
    "\n",
    "---\n",
    "## Project Description\n",
    "> In this project, we aim to develop a sophisticated **garbage classification system** leveraging the **EfficientNetV2B2** architecture. Our primary dataset serves as a foundation for building models that can eventually automate waste segregation, a critical step in optimizing recycling and waste management, ultimately aiding in environmental conservation.\n",
    "\n",
    "\n",
    "**Goal:** To develop an accurate and efficient garbage classification model using EfficientNetV2B2 and transfer learning for automated waste sorting.\n",
    "\n",
    "\n",
    "---\n",
    "## Challenges and Scope\n",
    "**Key Challenge:** A notable challenge encountered is the inherent **class imbalance** within the dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b116ab0-1fe1-4b26-8e46-e8b6764f5c14",
   "metadata": {
    "id": "9b116ab0-1fe1-4b26-8e46-e8b6764f5c14"
   },
   "source": [
    "![image.png](attachment:6297be3d-ff2c-4777-a77b-b80cb1775f11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8740a2a5-3b34-48d0-b0ce-4b82b21374f2",
   "metadata": {
    "id": "8740a2a5-3b34-48d0-b0ce-4b82b21374f2"
   },
   "source": [
    "\n",
    "    Transfer Learning is a machine learning technique where a pre-trained model developed for a specific task is reused as the starting point for a model on a different but related task. It also allows us to build accurate models in a time-saving way by starting from patterns learned when solving a different problem. This approach is beneficial when there is limited data for the new task, as the pre-trained model already has learned features that can be adapted. Transfer learning can significantly improve models' performance and efficiency in domains like computer vision and natural language processing.\n",
    "    \n",
    "### Benefits\n",
    "-  **Reduces training time** ‚Äî you don't start from scratch.\n",
    "-  **Leverages learned features** from large datasets (like ImageNet).\n",
    "-  **Improves performance**, especially with limited data.\n",
    "\n",
    "---\n",
    "\n",
    "##  How Does It Work?\n",
    "\n",
    "1.  Load a pretrained model (e.g., ResNet, EfficientNet).\n",
    "2.  **Freeze** the pretrained layers (optional).\n",
    "3.  Add new layers for your custom task.\n",
    "4.  Train on your new dataset (can also fine-tune).\n",
    "## EfficientNetV2B2: Transfer Learning Backbone\n",
    "\n",
    "EfficientNetV2B2 is a mid-sized model from the EfficientNetV2 family developed by **Google**, balancing performance and efficiency.\n",
    "\n",
    "### ‚öôÔ∏è Key Features:\n",
    "- **Fused MBConv blocks** ‚Äî enhance both training stability and speed.\n",
    "- **Progressive learning** ‚Äî enables better generalization with less computation.\n",
    "- **Improved architecture** ‚Äî achieves higher accuracy with optimized FLOPs.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use EfficientNetV2B2?\n",
    "\n",
    "| Feature                  | Description                                       |\n",
    "|--------------------------|---------------------------------------------------|\n",
    "| Balanced Performance    | Great trade-off between speed and accuracy        |\n",
    "| Scalable                 | Suitable for moderately complex datasets          |\n",
    "| Pretrained on ImageNet   | Solid backbone for transfer learning tasks        |\n",
    "| Efficient                | Faster convergence with fewer resources needed    |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EQ_b1fI6bEr2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "EQ_b1fI6bEr2",
    "outputId": "d3922f74-6372-4d94-c753-86d6ab6277a6"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "with zipfile.ZipFile(\"archive.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"garbage_data\")\n",
    "\n",
    "print(\"Extracted:\", os.listdir(\"garbage_data\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8cfac7-4e84-4a18-a24f-502198f5d13a",
   "metadata": {
    "id": "eb8cfac7-4e84-4a18-a24f-502198f5d13a"
   },
   "source": [
    "###  Core Libraries\n",
    "- `tensorflow`: For deep learning model building and training.\n",
    "- `numpy`: For numerical operations and array manipulation.\n",
    "- `matplotlib.pyplot`: For plotting training curves and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd00878-a85e-417d-8855-305b222bee01",
   "metadata": {
    "id": "cfd00878-a85e-417d-8855-305b222bee01"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np  # Importing NumPy for numerical operations and array manipulations\n",
    "import matplotlib.pyplot as plt  # Importing Matplotlib for plotting graphs and visualizations\n",
    "import seaborn as sns  # Importing Seaborn for statistical data visualization, built on top of Matplotlib\n",
    "import tensorflow as tf  # Importing TensorFlow for building and training machine learning models\n",
    "from tensorflow import keras  # Importing Keras, a high-level API for TensorFlow, to simplify model building\n",
    "from tensorflow.keras import Layer  # Importing Layer class for creating custom layers in Keras\n",
    "from tensorflow.keras.models import Sequential  # Importing Sequential model for building neural networks layer-by-layer\n",
    "from tensorflow.keras.layers import Rescaling , GlobalAveragePooling2D\n",
    "from tensorflow.keras import layers, optimizers, callbacks  # Importing various modules for layers, optimizers, and callbacks in Keras\n",
    "from sklearn.utils.class_weight import compute_class_weight  # Importing function to compute class weights for imbalanced datasets\n",
    "from tensorflow.keras.applications import EfficientNetV2B2  # Importing EfficientNetV2S model for transfer learning\n",
    "from sklearn.metrics import confusion_matrix, classification_report  # Importing functions to evaluate model performance\n",
    "import gradio as gr  # Importing Gradio for creating interactive web interfaces for machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4de6bd6-f331-4728-a369-43d450ad0c3e",
   "metadata": {
    "id": "e4de6bd6-f331-4728-a369-43d450ad0c3e"
   },
   "source": [
    "## 1.  Explore and Understand the Data\n",
    "- Load image dataset using tools like `image_dataset_from_directory`.\n",
    "- Visualize sample images from each class.\n",
    "- Check the number of images per class to ensure balance.\n",
    "- Understand image dimensions, color channels, and class labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1862c5-bf45-41a6-9797-efb618fa6dd8",
   "metadata": {
    "id": "bb1862c5-bf45-41a6-9797-efb618fa6dd8"
   },
   "source": [
    "\n",
    "### Load image dataset using tools like `image_dataset_from_directory`.\n",
    "### Split data into training, validation, and testing sets.\n",
    "\n",
    "`tf.keras.utils.image_dataset_from_directory(...)`  \n",
    "Used to load images from a directory where each subfolder represents a class.\n",
    "\n",
    "---\n",
    "\n",
    "**path**  \n",
    "Root directory path containing one subdirectory per class.\n",
    "\n",
    "**shuffle=True**  \n",
    "Randomly shuffles the image data. Useful during training to prevent the model from learning the order of the data.\n",
    "\n",
    "**image_size=(128, 128)**  \n",
    "Resizes all loaded images to this target size (width, height).  \n",
    "This must match the input size expected by the model.\n",
    "\n",
    "**batch_size=32**  \n",
    "Number of images per batch during training.  \n",
    "This affects memory usage and the frequency of model updates.\n",
    "\n",
    "**validation_split=False**  \n",
    "If set to a float (e.g., `0.2`), splits a portion of the data for validation.  \n",
    "If `False`, no split is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2cd363-b153-449d-902a-70788194b8b1",
   "metadata": {
    "id": "0f2cd363-b153-449d-902a-70788194b8b1"
   },
   "outputs": [],
   "source": [
    "base_dir = \"garbage_data/TrashType_Image_Dataset\"\n",
    "\n",
    "image_size = (124, 124)\n",
    "batch_size = 32\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfb0680-f38a-424d-816f-a58c52eba46a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbfb0680-f38a-424d-816f-a58c52eba46a",
    "outputId": "7867cf2c-bb19-4124-f1d3-f51cfa5c76a1"
   },
   "outputs": [],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    base_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=seed,\n",
    "    shuffle = True,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc253400-7514-413a-927b-1689252080fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc253400-7514-413a-927b-1689252080fc",
    "outputId": "5ae52ce2-bc3d-4ef9-c13d-2d12dac003fe"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    base_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=seed,\n",
    "    shuffle = True,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_class= val_ds.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d139fa27-d20d-485c-88e7-d8a14561b748",
   "metadata": {
    "id": "d139fa27-d20d-485c-88e7-d8a14561b748"
   },
   "outputs": [],
   "source": [
    "# Get the total number of batches in the validation dataset\n",
    "val_batches = tf.data.experimental.cardinality(val_ds)\n",
    "\n",
    "# Split the validation dataset into two equal parts:\n",
    "# First half becomes the test dataset\n",
    "test_ds = val_ds.take(val_batches // 2)\n",
    "\n",
    "# Second half remains as the validation dataset\n",
    "val_dat = val_ds.skip(val_batches // 2)\n",
    "\n",
    "# Optimize test dataset by caching and prefetching to improve performance\n",
    "test_ds_eval = test_ds.cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da996ee-6b15-4a5a-89c6-73013a3d3db8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6da996ee-6b15-4a5a-89c6-73013a3d3db8",
    "outputId": "c11c568e-55e6-4ead-f014-73f7e2adf052"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(train_ds.class_names)\n",
    "print(val_class)\n",
    "print(len(train_ds.class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5675405c-473d-4f86-a514-bfaf4a68edb7",
   "metadata": {
    "id": "5675405c-473d-4f86-a514-bfaf4a68edb7"
   },
   "source": [
    "### Visualize sample images from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbd2220-9c0e-485b-90e8-3ec86034c7a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 829
    },
    "id": "ffbd2220-9c0e-485b-90e8-3ec86034c7a5",
    "outputId": "d75ce2be-ae69-4a23-8118-8265d3c9f69a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(12):\n",
    "    ax = plt.subplot(4, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(train_ds.class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73855bf2-2c41-41d0-bd7b-b3eaff8d122a",
   "metadata": {
    "id": "73855bf2-2c41-41d0-bd7b-b3eaff8d122a"
   },
   "source": [
    "- ## Check the number of images per class to ensure balance\n",
    "- ## Understand image properties like Image dimensions, Class labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed9597d-b469-4302-ba28-91751cb548b6",
   "metadata": {
    "id": "1ed9597d-b469-4302-ba28-91751cb548b6"
   },
   "outputs": [],
   "source": [
    "def count_distribution(dataset, class_names):\n",
    "    total = 0\n",
    "    counts = {name: 0 for name in class_names}\n",
    "\n",
    "    for _, labels in dataset:\n",
    "        for label in labels.numpy():\n",
    "            class_name = class_names[label]\n",
    "            counts[class_name] += 1\n",
    "            total += 1\n",
    "\n",
    "    for k in counts:\n",
    "        counts[k] = round((counts[k] / total) * 100, 2)  # Convert to percentage\n",
    "    return counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a75d5-11c6-4d63-ab77-a787969dfd14",
   "metadata": {
    "id": "186a75d5-11c6-4d63-ab77-a787969dfd14"
   },
   "outputs": [],
   "source": [
    "# Function to plot class distribution\n",
    "def simple_bar_plot(dist, title):\n",
    "    plt.bar(dist.keys(), dist.values(), color='cornflowerblue')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Percentage (%)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 100)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0651802-9d3b-416d-ac22-4b13d400b43d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0651802-9d3b-416d-ac22-4b13d400b43d",
    "outputId": "12cd7dd0-f07b-4ddc-d4ef-b38fc7094f0c"
   },
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "\n",
    "# Get class distributions\n",
    "train_dist = count_distribution(train_ds, class_names)\n",
    "val_dist = count_distribution(val_ds, class_names)\n",
    "test_dist = count_distribution(test_ds, class_names)\n",
    "overall_dist = {}\n",
    "for k in class_names:\n",
    "    overall_dist[k] = round((train_dist[k] + val_dist[k]) / 2, 2)\n",
    "\n",
    "print(train_dist)\n",
    "print(val_dist)\n",
    "print(test_dist)\n",
    "print(overall_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc59a74-2686-4036-b7a4-6fb59265009c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ffc59a74-2686-4036-b7a4-6fb59265009c",
    "outputId": "92b0df6d-67a1-4095-e960-5953bb67e0f4"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Show visualizations\n",
    "simple_bar_plot(train_dist, \"Training Set Class Distribution (%)\")\n",
    "simple_bar_plot(val_dist, \"Validation Set Class Distribution (%)\")\n",
    "simple_bar_plot(test_dist, \"Test Set Class Distribution (%)\")\n",
    "simple_bar_plot(overall_dist, \"Overall Class Distribution (%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eafd2a-9ca7-41a3-8e29-cc2a9ed2d374",
   "metadata": {
    "id": "f7eafd2a-9ca7-41a3-8e29-cc2a9ed2d374"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "#  Inference on Class Imbalance\n",
    "\n",
    "The **\"Garbage Image Dataset\"** reveals a noticeable **imbalance** in the distribution of its image categories:\n",
    "\n",
    "| Category     | Image Count | Updated Distribution |\n",
    "|--------------|-------------|----------------|\n",
    "| Cardboard | 403          | 15.09           |\n",
    "|  Glass     | 501         | 19.96           |\n",
    "|  Metal     | 410         | 16.68           |\n",
    "|  Paper     | 594         | 23.82          |\n",
    "|  Plastic   | 482         | 18.53         |\n",
    "|  Trash     | 137         |  5.91          |\n",
    "\n",
    "---\n",
    "\n",
    "### Analogy:\n",
    "> Imagine teaching a child to identify animals by showing them **95 pictures of cats** and just **5 pictures of dogs**.  \n",
    "> They'd probably think **most pets are cats**, right?  \n",
    "> Similarly, our model sees a lot of \"**paper**\" and very little \"**trash**\", which **biases** its understanding.\n",
    "\n",
    "---\n",
    "\n",
    "###  Key Problems Caused by Class Imbalance:\n",
    "\n",
    "#### 1Ô∏è **Bias**\n",
    "- The model may **overpredict common classes** like `\"paper\"` and **underpredict rare ones** like `\"trash\"`.\n",
    "\n",
    "#### 2Ô∏è **Generalization Issues**\n",
    "- If the real-world distribution is more balanced, the model may **fail to generalize** and **misclassify rare classes**.\n",
    "\n",
    "#### 3Ô∏è **Accuracy Deception**\n",
    "- The model might appear to have **high overall accuracy** just by **predicting the majority class**, while **failing** on underrepresented ones.\n",
    "\n",
    "---\n",
    "\n",
    "###  Solution Approaches :\n",
    "- Use **class weights** to handle imbalanced data in training,\n",
    "- Apply **data augmentation** to increase training data diversity\n",
    "---\n",
    "\n",
    " **Conclusion**: Always check class distribution. A seemingly \"accurate\" model might just be **biased** toward the dominant class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39bed4-d41c-4fa9-8433-646fbe103302",
   "metadata": {
    "id": "6e39bed4-d41c-4fa9-8433-646fbe103302"
   },
   "source": [
    "\n",
    "### üõ†Ô∏è Addressing Imbalance Using Class Weights:\n",
    "\n",
    "To tackle our imbalanced image dataset, we'll utilize **class weights**. These weights assign more importance to underrepresented classes during training. The weights are computed inversely proportional to class frequencies using utilities like `compute_class_weight` from **scikit-learn**, based on the distribution of images in each class. The formula is:\n",
    "\n",
    "$$\n",
    "\\text{weight(class)} = \\frac{\\text{total samples}}{\\text{number of classes} \\times \\text{samples in that class}}\n",
    "$$\n",
    "\n",
    "These computed weights are then passed to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af495a1b-a1c6-4206-8c55-8e0f6347e733",
   "metadata": {
    "id": "af495a1b-a1c6-4206-8c55-8e0f6347e733"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Count class occurrences and prepare label list\n",
    "class_counts = {i: 0 for i in range(len(class_names))}\n",
    "all_labels = []\n",
    "\n",
    "for images, labels in train_ds:\n",
    "    for label in labels.numpy():\n",
    "        class_counts[label] += 1\n",
    "        all_labels.append(label)\n",
    "\n",
    "# Compute class weights (index aligned)\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(len(class_names)),\n",
    "    y=all_labels\n",
    ")\n",
    "\n",
    "# Create dictionary mapping class index to weight\n",
    "class_weights = {i: w for i, w in enumerate(class_weights_array)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311ae34-edda-421f-a693-ce278220efac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8311ae34-edda-421f-a693-ce278220efac",
    "outputId": "e688d911-b001-482e-978b-594c8398366d"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ‚úÖ Optional: print results\n",
    "print(\"Class Counts:\", class_counts)\n",
    "print(\"Class Weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a920855-e4ed-41da-a7fc-edba08879a09",
   "metadata": {
    "id": "2a920855-e4ed-41da-a7fc-edba08879a09"
   },
   "source": [
    "\n",
    "## 2.  Data Preprocessing / Preparation\n",
    "- Resize and rescale images.\n",
    "- Apply data augmentation (e.g., `RandomFlip`, `RandomRotation`, `RandomZoom`) to improve generalization.\n",
    "- Normalize images (using `preprocess_input` if using pre-trained models like EfficientNet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e74646-16d3-414f-8013-4a4168b8ff1c",
   "metadata": {
    "id": "b7e74646-16d3-414f-8013-4a4168b8ff1c"
   },
   "outputs": [],
   "source": [
    "#  Define data augmentation pipeline\n",
    "data_augmentation = Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomContrast(0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b417dc72-7946-4650-875d-ac4d278db425",
   "metadata": {
    "id": "b417dc72-7946-4650-875d-ac4d278db425"
   },
   "source": [
    "\n",
    "## 3.  Model Selection\n",
    "- Choose a base model: Custom CNN or Transfer Learning (e.g., `EfficientNetV2B2`).\n",
    "- Decide whether to use pre-trained weights (e.g., ImageNet).\n",
    "- Define whether layers should be trainable or frozen during initial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cbff2e-8132-4d6d-b4cb-db92088f7cd6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0cbff2e-8132-4d6d-b4cb-db92088f7cd6",
    "outputId": "7277b88e-fd30-4591-81af-1b80620942ee"
   },
   "outputs": [],
   "source": [
    "#  Load the pretrained MobileNetV3Small model (without the top classification layer)\n",
    "base_model = EfficientNetV2B2(include_top=False, input_shape=(124, 124, 3),include_preprocessing=True, weights='imagenet')\n",
    "\n",
    "\n",
    "#  Freeze early layers (to retain general pretrained features)\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:100]:  # You can adjust this number\n",
    "    layer.trainable = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87504bf-359b-4eaa-b174-a5e3d97e038c",
   "metadata": {
    "id": "a87504bf-359b-4eaa-b174-a5e3d97e038c"
   },
   "source": [
    "\n",
    "## 4.  Model Training\n",
    "- Build the model architecture using `Sequential` or Functional API.\n",
    "- Compile the model with loss function ( `sparse_categorical_crossentropy`), optimizer (e.g., `Adam`), and evaluation metrics (`accuracy`).\n",
    "\n",
    "## 5.  Model Tuning and Optimization\n",
    "- Tune hyperparameters: learning rate, batch size, number of layers, dropout rate.\n",
    "- Use callbacks: `EarlyStopping`,\n",
    "- Optionally perform fine-tuning on pre-trained models by unfreezing some layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe324b-1a18-4c96-a2f2-c5b7cd797c52",
   "metadata": {
    "id": "b8fe324b-1a18-4c96-a2f2-c5b7cd797c52"
   },
   "source": [
    "### Model Architecture and Layer Utilities\n",
    "\n",
    "- **Sequential**: A simple way to build models by stacking layers one after the other in a linear fashion.\n",
    "\n",
    "- **RandomFlip**: A data augmentation layer that flips input images horizontally or vertically at random, helping the model generalize better.\n",
    "\n",
    "- **RandomRotation**: Randomly rotates images by a specified angle range during training to make the model invariant to orientation.\n",
    "\n",
    "- **RandomZoom**: Applies random zoom-in or zoom-out to training images, helping the model recognize objects at various scales.\n",
    "\n",
    "- **Dropout**: A regularization method that randomly \"drops\" (sets to zero) a fraction of input units during training to prevent overfitting.\n",
    "\n",
    "- **GlobalAveragePooling2D**: Reduces each feature map to a single number by taking the average, reducing model parameters and helping prevent overfitting.\n",
    "\n",
    "- **Dense**: A fully connected neural network layer used to learn complex features and typically found at the end of the model for classification.\n",
    "\n",
    "- **Input**: Specifies the input shape and data type for the model; acts as the starting point of the model architecture.\n",
    "\n",
    "- **EfficientNetV2B2**: A pre-trained convolutional neural network from the EfficientNetV2 family, known for being lightweight and high-performing, commonly used for transfer learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d124d5bf-b95c-49dd-a5c8-8ed02e9835dd",
   "metadata": {
    "id": "d124d5bf-b95c-49dd-a5c8-8ed02e9835dd"
   },
   "outputs": [],
   "source": [
    "#  Build the final model\n",
    "model = Sequential([\n",
    "    layers.Input(shape=(124, 124, 3)),\n",
    "    data_augmentation,\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(6, activation='softmax')  # Change to your number of classes\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d39a5-c6f2-42d8-b04d-678e7c1b6914",
   "metadata": {
    "id": "6e7d39a5-c6f2-42d8-b04d-678e7c1b6914"
   },
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Compile the model\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80608f99-b04d-430e-94b0-8e39f0c95cb0",
   "metadata": {
    "id": "80608f99-b04d-430e-94b0-8e39f0c95cb0"
   },
   "source": [
    "### üîÅ Callbacks\n",
    "- `EarlyStopping`: To stop training when validation performance stops improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a8ba6-4c03-42e6-ad19-0465ee2c379a",
   "metadata": {
    "id": "451a8ba6-4c03-42e6-ad19-0465ee2c379a"
   },
   "outputs": [],
   "source": [
    "# Define an EarlyStopping callback to stop training when validation loss stops improving\n",
    "early = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',            # Metric to monitor (validation loss here)\n",
    "    patience=3,                   # Number of epochs to wait after last improvement before stopping\n",
    "    restore_best_weights=True     # After stopping, restore the model weights from the epoch with the best val_loss\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394ae8ba-d77b-4b4f-85a2-c3ed89b75cb4",
   "metadata": {
    "id": "394ae8ba-d77b-4b4f-85a2-c3ed89b75cb4"
   },
   "source": [
    "### Train the model using `.fit()` with appropriate `epochs`, `batch_size`, and callbacks like `EarlyStopping`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0011e7c8-d61e-4106-8cd6-48df88e522af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0011e7c8-d61e-4106-8cd6-48df88e522af",
    "outputId": "63873b70-3344-4f90-db79-e8648a7ebce7"
   },
   "outputs": [],
   "source": [
    "# Set the number of epochs to train the model\n",
    "epochs = 15  # Number of times the model will go through the entire dataset\n",
    "\n",
    "# Train the model using the fit function\n",
    "history = model.fit(\n",
    "    train_ds,                # Training dataset used to adjust model weights\n",
    "    validation_data=val_ds,   # Validation dataset to monitor performance on unseen data\n",
    "    epochs=epochs,           # Number of training cycles, referencing the variable set earlier\n",
    "    class_weight=class_weights,  # Handles class imbalances by assigning appropriate weights\n",
    "    batch_size=32,           # Number of samples processed in each training step\n",
    "    callbacks=[early]        # Implements early stopping to prevent unnecessary training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb728268-56a3-44fb-ad76-2c17ec120c5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "bb728268-56a3-44fb-ad76-2c17ec120c5c",
    "outputId": "e1270bcd-a081-40e8-f48a-d130966f5899"
   },
   "outputs": [],
   "source": [
    "# üìù Summary (optional but useful)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6f5d06-7884-49ba-b74f-99fa18ee6c69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7a6f5d06-7884-49ba-b74f-99fa18ee6c69",
    "outputId": "76fcb8e2-379a-40d3-9f67-28c4a3055757"
   },
   "outputs": [],
   "source": [
    "base_model.summary() # Print the architecture summary of the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b800f503-2843-4b5a-ade9-3ebf7c01ecd6",
   "metadata": {
    "id": "b800f503-2843-4b5a-ade9-3ebf7c01ecd6"
   },
   "source": [
    "# Model Performance Visualization: Accuracy & Loss Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf163b9-3b14-422c-a24e-2abbf7dc8989",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "id": "ebf163b9-3b14-422c-a24e-2abbf7dc8989",
    "outputId": "571a74aa-34bc-4b57-8255-d6b296883346"
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']          # Extract training accuracy from history\n",
    "val_acc = history.history['val_accuracy']  # Extract validation accuracy from history\n",
    "loss = history.history['loss']             # Extract training loss from history\n",
    "val_loss = history.history['val_loss']     # Extract validation loss from history\n",
    "\n",
    "epochs_range = range(len(acc))             # Define range for epochs based on accuracy length\n",
    "\n",
    "plt.figure(figsize=(10,8))                 # Set overall figure size for visualization\n",
    "\n",
    "plt.subplot(1,2,1)                         # Create first subplot (1 row, 2 columns, position 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')       # Plot training accuracy\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy') # Plot validation accuracy\n",
    "plt.legend(loc='lower right')              # Place legend in lower-right corner\n",
    "plt.title('Training vs Validation Accuracy') # Add title for accuracy plot\n",
    "\n",
    "plt.subplot(1,2,2)                         # Create second subplot (1 row, 2 columns, position 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')         # Plot training loss\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')   # Plot validation loss\n",
    "plt.legend(loc='upper right')              # Place legend in upper-right corner\n",
    "plt.title('Training vs Validation Loss')   # Add title for loss plot\n",
    "\n",
    "plt.show()                                 # Display the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a88c4-c39f-4212-be6b-3d99212f7b12",
   "metadata": {
    "id": "081a88c4-c39f-4212-be6b-3d99212f7b12"
   },
   "source": [
    "## 5.  Model Evaluation\n",
    "- Plot training and validation accuracy/loss curves.\n",
    "- Evaluate model performance on validation or test set.\n",
    "- Use metrics like:\n",
    "  - **Confusion Matrix**\n",
    "  - **Classification Report** (Precision, Recall, F1-score)\n",
    "  - `confusion_matrix`, `classification_report`: To evaluate the model's classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc38341-3066-46bb-80b3-e164b1739cc9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbc38341-3066-46bb-80b3-e164b1739cc9",
    "outputId": "688ce3e2-f796-4bf2-9800-287399783a23"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_ds_eval)\n",
    "print(f'Test accuracy is{accuracy:.4f}, Test loss is {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfdce18-115c-428b-9246-8a1f5cd94f06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bfdce18-115c-428b-9246-8a1f5cd94f06",
    "outputId": "b61a376a-e6c9-41c8-8283-ef86a4fd8f83"
   },
   "outputs": [],
   "source": [
    "# Extract true labels from all batches in the test dataset\n",
    "y_true = np.concatenate([y.numpy() for x, y in test_ds_eval], axis=0)  # Convert Tensor labels to NumPy array and concatenate them\n",
    "\n",
    "# Get predictions as probabilities from the model\n",
    "y_pred_probs = model.predict(test_ds_eval)  # Predict class probabilities for each sample in the test dataset\n",
    "\n",
    "# Convert probabilities to predicted class indices\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # Select the class with the highest probability for each sample\n",
    "\n",
    "# Compute the confusion matrix to evaluate classification performance\n",
    "cm = confusion_matrix(y_true, y_pred)  # Generate confusion matrix comparing true labels to predicted labels\n",
    "\n",
    "# Print metrics to assess model performance\n",
    "print(cm)  # Display confusion matrix\n",
    "print(classification_report(y_true, y_pred))  # Print precision, recall, and F1-score for each class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d10b303-4f28-4b6b-9d5c-f89f4ba96cc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "9d10b303-4f28-4b6b-9d5c-f89f4ba96cc3",
    "outputId": "179e8321-17d7-4303-a37c-b53af950d52d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))  # Set figure size for better visualization\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d',  # Create heatmap using confusion matrix\n",
    "            xticklabels=class_names,  # Set class names for x-axis (predicted labels)\n",
    "            yticklabels=class_names,  # Set class names for y-axis (true labels)\n",
    "            cmap='Blues')  # Use a blue colormap for better contrast\n",
    "\n",
    "plt.xlabel('Predicted')  # Label x-axis as Predicted classes\n",
    "plt.ylabel('True')  # Label y-axis as True classes\n",
    "plt.title('Confusion Matrix')  # Add title to the heatmap\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44202923-4802-448b-b322-7d742b55bfa6",
   "metadata": {
    "id": "44202923-4802-448b-b322-7d742b55bfa6"
   },
   "source": [
    "## 7.  Final Testing and Save the Model\n",
    "- Evaluate the final model on the unseen **test dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d925a-0ca8-4e03-9cc9-5b5a9d8b4103",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "863d925a-0ca8-4e03-9cc9-5b5a9d8b4103",
    "outputId": "60b332c7-5691-4fe0-8d51-108c1c8d9f00"
   },
   "outputs": [],
   "source": [
    "# Extract class names from the training dataset\n",
    "class_names = train_ds.class_names\n",
    "\n",
    "# Take one batch of images and labels from the test dataset for evaluation\n",
    "for images, labels in test_ds_eval.take(1):\n",
    "\n",
    "    # Generate predictions for the batch of images\n",
    "    predictions = model.predict(images)\n",
    "\n",
    "    # Get the predicted class index for each image\n",
    "    pred_labels = tf.argmax(predictions, axis=1)\n",
    "\n",
    "    # Loop through the first 8 images in the batch\n",
    "    for i in range(8):\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))  # Convert and display image\n",
    "        plt.title(f\"True: {class_names[labels[i]]}, Pred: {class_names[pred_labels[i]]}\")  # Show actual and predicted class\n",
    "        plt.axis(\"off\")  # Hide axes for better visualization\n",
    "        plt.show()  # Display the image with title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aff71bd-bab7-480f-b57f-2416b8d4071a",
   "metadata": {
    "id": "8aff71bd-bab7-480f-b57f-2416b8d4071a"
   },
   "source": [
    "**Save the trained model using `model.save()` or `save_model()` for future inference.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf4d17c-f84e-4ca3-8355-35c6ea81d47e",
   "metadata": {
    "id": "5bf4d17c-f84e-4ca3-8355-35c6ea81d47e"
   },
   "outputs": [],
   "source": [
    "# Save model in Keras format with architecture, weights, and training configuration\n",
    "model.save('Effiicientnetv2b2.keras')\n",
    "\n",
    "# Load your Keras model\n",
    "model = tf.keras.models.load_model('Effiicientnetv2b2.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8755a4fe-818b-40d1-b385-e6a3e21696d5",
   "metadata": {
    "id": "8755a4fe-818b-40d1-b385-e6a3e21696d5"
   },
   "source": [
    "\n",
    "## 8.  Model Deployment (Optional)\n",
    "- Create a web interface using **Gradio**.\n",
    "- Load the saved model and preprocess input images before prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c9067b-ed30-4855-998c-622bd6bfb57c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2c9067b-ed30-4855-998c-622bd6bfb57c",
    "outputId": "e4d6ac93-6acb-42c1-d53e-d28fa0c175cd"
   },
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbee0db0-516d-494a-8612-8bd2c315eca4",
   "metadata": {
    "id": "dbee0db0-516d-494a-8612-8bd2c315eca4"
   },
   "source": [
    "### üåê Gradio Interface and Preprocessing\n",
    "- `gr`: To build a web interface for the model.\n",
    "- `PIL.Image`: For handling image input in Gradio.\n",
    "- `preprocess_input`: Preprocessing method for EfficientNet.\n",
    "- `load_model`: For loading a saved model for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f2e86d-1064-4481-8e05-486cbdaa8177",
   "metadata": {
    "id": "65f2e86d-1064-4481-8e05-486cbdaa8177"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7f957-d903-44ff-9f19-c4a25c4e212f",
   "metadata": {
    "id": "94e7f957-d903-44ff-9f19-c4a25c4e212f"
   },
   "outputs": [],
   "source": [
    "def classify_image(img):\n",
    "    # Resize image to 124x124 pixels (Note: Comment says 128x128, but code resizes to 124x124)\n",
    "    img = img.resize((124, 124))\n",
    "\n",
    "    # Convert image to a NumPy array with float32 dtype\n",
    "    img_array = np.array(img, dtype=np.float32)\n",
    "    img_array = preprocess_input(img_array)\n",
    "\n",
    "    # Expand dimensions to match model input shape (adds a batch dimension)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    # Make a prediction using the trained model\n",
    "    prediction = model.predict(img_array)\n",
    "\n",
    "    # Get the index of the highest predicted probability\n",
    "    predicted_class_index = np.argmax(prediction)\n",
    "\n",
    "    # Map the predicted index to its corresponding class name\n",
    "    predicted_class_name = class_names[predicted_class_index]\n",
    "\n",
    "    # Extract confidence score (probability of the predicted class)\n",
    "    confidence = prediction[0][predicted_class_index]\n",
    "\n",
    "    # Return formatted prediction result with confidence score\n",
    "    return f\"Predicted: {predicted_class_name} (Confidence: {confidence:.2f})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1632d69-dad1-4272-af0a-b49b4ee35e42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "d1632d69-dad1-4272-af0a-b49b4ee35e42",
    "outputId": "7e2c9f96-5a0b-459e-8fbe-c23079b2eadd"
   },
   "outputs": [],
   "source": [
    "iface = gr.Interface(\n",
    "    fn=classify_image,  # Function to classify image using the trained model\n",
    "    inputs=gr.Image(type=\"pil\"),  # Accepts input as a PIL image\n",
    "    outputs=\"text\"  # Outputs prediction as text\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()  # Start the Gradio interface for user interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f393fc-4397-48b1-8755-e24fe3082e07",
   "metadata": {
    "id": "01f393fc-4397-48b1-8755-e24fe3082e07"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "The image classification model demonstrates strong accuracy in identifying objects, leveraging deep learning to refine predictions effectively. Its robust performance ensures reliable classification, making it a valuable tool for various applications."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
